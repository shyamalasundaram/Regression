{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#**Supervised Learning:** **Regression Models and Performance Metrics**"
      ],
      "metadata": {
        "id": "R1zBxUr9_oSn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Question 1 : What is Simple Linear Regression (SLR)? Explain its purpose**\n",
        "\n",
        "**Answer:**  \n",
        "###**Definition:**  \n",
        "Simple Linear Regression (SLR) is a statistical method used to model the relationship between two continuous variables. In SLR, one variable is considered the predictor (or independent variable), and the other is the response (or dependent variable).\n",
        "###**Purpose:**\n",
        "The purpose of SLR is to create a linear equation that best predicts the value of the response variable based on the value of the predictor variable. The equation typically looks like \\(Y = \\beta_0 + \\beta_1X + \\epsilon\\)\\, where:\n",
        "- \\(Y\\) is the response variable.\n",
        "- \\(X\\) is the predictor variable.\n",
        "- \\(\\beta_0\\) is the intercept.\n",
        "- \\(\\beta_1\\) is the slope of the line.\n",
        "- \\(\\epsilon\\) is the error term.\n"
      ],
      "metadata": {
        "id": "JGAmTxokAbi7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Question 2: What are the key assumptions of Simple Linear Regression?**\n",
        "\n",
        "###**Answer:**\n",
        "###**Key Assumptions of Simple Linear Regression:**\n",
        "1. **Linearity:** The relationship between the predictor variable \\(X\\) and the response variable \\(Y\\) is linear.\n",
        "2. **Independence:** Observations are independent of each other.\n",
        "3. **Homoscedasticity:** The residuals have constant variance across all levels of \\(X\\).\n",
        "4. **Normality of Residuals:** The residuals are normally distributed.\n",
        "5. **No or little multicollinearity:** Not a major concern in SLR since there's only one predictor, but relevant for multiple regression.\n",
        "6. **No significant outliers:** Outliers can unduly influence the model fit.\n",
        "\n"
      ],
      "metadata": {
        "id": "5cKc-XNBC0VM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Question 3: Write the mathematical equation for a simple linear regression model and explain each term.**\n",
        "\n",
        "###**Answer:**\n",
        "\n",
        "The mathematical equation for a simple linear regression model is **\\(y=a+bx\\)**, where **'\\(y\\)'** is the dependent variable, **'\\(x\\)'** is the independent variable, **'\\(a\\)'** is the y-intercept, and **'\\(b\\)'** is the slope of the line.   \n",
        "The equation represents the linear relationship between two variables, with '\\(b\\)' indicating how much '\\(y\\)' changes for every one-unit increase in '\\(x\\)', and '\\(a\\)' representing the value of '\\(y\\)' when '\\(x\\) is zero.           \n",
        "###**Equation:**                 \n",
        "##**y = a + bx**"
      ],
      "metadata": {
        "id": "lVsFShhRD6In"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6315527d"
      },
      "source": [
        "##**Question 4: Provide a real-world example where simple linear regression can be applied.**\n",
        "\n",
        "###**Answer:**\n",
        "\n",
        "A classic example is predicting the **selling price of a house** based on its **square footage**. In this case:\n",
        "\n",
        "*   **Response Variable (y):** Selling Price of the house.\n",
        "*   **Predictor Variable (x):** Square footage of the house.\n",
        "\n",
        "We would collect data on the selling prices and square footage of many houses. Then, we could use simple linear regression to find the linear relationship between these two variables. The resulting equation would allow us to predict the selling price of a new house based on its square footage."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7c09e94f"
      },
      "source": [
        "##**Question 5: What is the method of least squares in linear regression?**\n",
        "\n",
        "###**Answer:**\n",
        "\n",
        "The **method of least squares** is a standard approach in simple linear regression (and other regression models) to find the best-fitting line through a set of data points.\n",
        "\n",
        "The goal is to minimize the **sum of the squared differences** between the observed values of the dependent variable (y) and the values predicted by the linear model. These differences are called **residuals**.\n",
        "\n",
        "In essence, the method of least squares finds the values for the intercept (\\(a\\)) and the slope (\\(b\\)) in the equation \\(y = a + bx\\) that result in the smallest possible sum of the squared residuals. By minimizing the squared errors, the method ensures that the fitted line is as close as possible to all the data points, giving more weight to larger errors."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Question 6: What is Logistic Regression? How does it differ from Linear Regression?**\n",
        "###**Answer:**\n",
        "\n",
        "**Logistic regression** is a statistical and machine learning method used for binary classification, predicting the probability of a categorical outcome. While it has \"regression\" in its name, its purpose is to classify data points into discrete categories, such as yes/no, pass/fail, or 0/1.    \n",
        "\n",
        "The key to logistic regression is the sigmoid (or logistic) function, which maps any real-valued number into a value between 0 and 1. This S-shaped curve represents the probability of a given input belonging to a specific class. A threshold (usually 0.5) is then used to decide the final class label.\n",
        "\n",
        "*Here are the specific differences between Logistic and Linear Regression in a table format:*\n",
        "\n",
        "| Feature            | Linear Regression             | Logistic Regression           |\n",
        "| :----------------- | :---------------------------- | :---------------------------- |\n",
        "| **Dependent Variable** | Continuous                    | Categorical (e.g., binary)  |\n",
        "| **Output**         | Continuous Value              | Probability (0 to 1)          |\n",
        "| **Equation**       | \\(y = a + bx\\)                | Uses Sigmoid Function         |\n",
        "| **Purpose**        | Regression (Predicting value) | Classification (Predicting category) |\n",
        "| **Relationship**   | Linear                        | Non-linear (through sigmoid)  |"
      ],
      "metadata": {
        "id": "mPXDZFeaDENF"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1418a3ff"
      },
      "source": [
        "##**Question 7: Name and briefly describe three common evaluation metrics for regression models.**\n",
        "###**Answer:**\n",
        "\n",
        "Here are three common evaluation metrics for regression models:\n",
        "\n",
        "1.  **Mean Absolute Error (MAE):**\n",
        "    *   **Description:** MAE measures the average magnitude of the errors in a set of predictions, without considering their direction. It's the average of the absolute differences between the predicted and actual values.\n",
        "    *   **Formula:**  MAE =\\(\\frac{1}{N}\\sum _{i=1}^{N}|Y_{i}-\\^{Y}_{i}|\\)\n",
        "\n",
        "    *   **Interpretation:** A lower MAE indicates a better model fit. It's less sensitive to outliers than MSE.\n",
        "\n",
        "2.  **Mean Squared Error (MSE):**\n",
        "    *   **Description:** MSE measures the average of the squared errors. It's the sum of the squared differences between the predicted and actual values, divided by the number of observations.\n",
        "    *   **Formula:** \\( MSE = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 \\)\n",
        "    *   **Interpretation:** A lower MSE indicates a better model fit. It penalizes larger errors more heavily than MAE due to the squaring.\n",
        "\n",
        "3.  **R-squared (\\(R^2\\))**\n",
        "    *   **Description:** R-squared, also known as the coefficient of determination, represents the proportion of the variance in the dependent variable that is predictable from the independent variable(s).\n",
        "    *   **Formula:** \\( R^2 = 1 - \\frac{SSE}{SST} \\), where SSE is the sum of squared errors (residuals) and SST is the total sum of squares.\n",
        "    *   **Interpretation:** R-squared values range from 0 to 1. A higher R-squared indicates that a larger proportion of the variance is explained by the model, suggesting a better fit. An R-squared of 1 means the model perfectly predicts the dependent variable."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a7b9535e"
      },
      "source": [
        "##**Question 8: What is the purpose of the R-squared metric in regression analysis?**\n",
        "###**Answer:**\n",
        "\n",
        "The purpose of the **R-squared (\\(R^2\\))** metric in regression analysis is to measure how well the independent variable(s) explain the variation in the dependent variable. It quantifies the proportion of the variance in the dependent variable that is predictable from the independent variable(s) in the model.\n",
        "\n",
        "*   A high R-squared value (closer to 1) indicates that a large proportion of the variance in the dependent variable is explained by the model, suggesting a good fit.\n",
        "*   A low R-squared value (closer to 0) suggests that the model does not explain much of the variance in the dependent variable.\n",
        "\n",
        "In simpler terms, R-squared tells you how much of the \"spread\" or variability in the dependent variable is accounted for by the independent variable(s) in your regression model. It helps assess the overall goodness-of-fit of the model."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Question 9: Write Python code to fit a simple linear regression model using scikit-learn and print the slope and intercept.**\n"
      ],
      "metadata": {
        "id": "9AI5pxGTYodK"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "b4bdd915",
        "outputId": "0c1fc9ee-6c37-41d1-da1f-57aecb50dbd7"
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# Create a simple example dataset\n",
        "# X is the independent variable (feature)\n",
        "# y is the dependent variable (target)\n",
        "X = np.array([1, 2, 3, 4, 5]).reshape(-1, 1) # Reshape for scikit-learn\n",
        "y = np.array([2, 4, 5, 4, 5])\n",
        "\n",
        "# Create a Linear Regression model\n",
        "model = LinearRegression()\n",
        "\n",
        "# Fit the model to the data\n",
        "model.fit(X, y)\n",
        "\n",
        "# Print the slope and intercept\n",
        "print(\"Slope (Coefficient):\", model.coef_[0])\n",
        "print(\"Intercept:\", model.intercept_)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Slope (Coefficient): 0.6\n",
            "Intercept: 2.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6f24cf17"
      },
      "source": [
        "##**Question 10: How do you interpret the coefficients in a simple linear regression model?**\n",
        "\n",
        "###**Answer:**\n",
        "\n",
        "In a simple linear regression model with the equation \\(y = a + bx\\), there are two coefficients to interpret: the intercept (\\(a\\)) and the slope (\\(b\\)).\n",
        "\n",
        "1.  **Intercept (\\(a\\)):**\n",
        "    *   **Interpretation:** The intercept represents the predicted value of the dependent variable (\\(y\\)) when the independent variable (\\(x\\)) is zero.\n",
        "    *   **Context:** It's important to consider if \\(x=0\\) is a meaningful value in your dataset. In some cases, the intercept may not have a practical interpretation (e.g., predicting house price based on square footage, where square footage cannot be zero).\n",
        "\n",
        "2.  **Slope (\\(b\\)):**\n",
        "    *   **Interpretation:** The slope represents the change in the predicted value of the dependent variable (\\(y\\)) for every one-unit increase in the independent variable (\\(x\\)).\n",
        "    *   **Context:** The sign of the slope indicates the direction of the relationship (positive slope means \\(y\\) increases as \\(x\\) increases, negative slope means \\(y\\) decreases as \\(x\\) increases). The magnitude of the slope indicates the strength of the linear relationship.\n",
        "\n",
        "**Example (using the previous code's output):**\n",
        "\n",
        "From the previous code, we got a slope of 0.6 and an intercept of 2.2.\n",
        "\n",
        "*   **Intercept (2.2):** When the independent variable \\(X\\) is 0, the predicted value of the dependent variable \\(y\\) is 2.2.\n",
        "*   **Slope (0.6):** For every one-unit increase in \\(X\\), the predicted value of \\(y\\) increases by 0.6.\n",
        "\n",
        "It's crucial to interpret these coefficients within the context of your specific data and problem."
      ]
    }
  ]
}